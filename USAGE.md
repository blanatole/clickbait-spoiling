# Clickbait Spoiling Implementation

## ğŸ“‹ TÃ³m táº¯t

ÄÃ¢y lÃ  implementation cá»§a bÃ i bÃ¡o clickbait spoiling sá»­ dá»¥ng GPT-2 medium vá»›i cÃ¡c hyperparameters Ä‘Æ°á»£c tá»‘i Æ°u hÃ³a dá»±a trÃªn phÃ¢n tÃ­ch nghiÃªn cá»©u.

### ğŸ¯ Hyperparameters chÃ­nh (theo paper):
- **Model**: GPT-2 medium (~355M parameters)
- **Input**: postText + targetParagraph (max 512 tokens)
- **Loss**: Sparse categorical cross-entropy (causal LM)
- **Epochs**: 10
- **Decoding**: Conservative (greedy/small beam) Ä‘á»ƒ Ä‘áº¡t BLEU cao

## ğŸš€ CÃ i Ä‘áº·t

```bash
# CÃ i Ä‘áº·t dependencies
pip install -r requirements.txt

# Download NLTK data (náº¿u cáº§n)
python -c "import nltk; nltk.download('punkt')"
```

## ğŸ“Š Cáº¥u trÃºc dá»¯ liá»‡u

Dataset SemEval-2023 vá»›i format:
- **3,200 máº«u training** (train.jsonl)
- **800 máº«u validation** (validation.jsonl)
- Má»—i máº«u chá»©a: postText, targetParagraphs, spoiler, tags

## ğŸ”§ Sá»­ dá»¥ng

### 1. Training Model (Fine-tuning GPT-2)

```bash
# Training Ä‘áº§y Ä‘á»§ vá»›i hyperparameters tá»‘i Æ°u
cd clickbait-spoiling
python clickbait_spoiling.py
```

**Hyperparameters Ä‘Æ°á»£c sá»­ dá»¥ng:**
- Learning rate: 5e-5 (conservative)
- Batch size: 4 (vá»›i gradient accumulation = 2)
- Scheduler: Cosine decay
- Epochs: 10 (nhÆ° paper)
- Early stopping trÃªn validation BLEU

### 2. Quick Testing (KhÃ´ng cáº§n training)

```bash
# Test vá»›i pre-trained GPT-2 medium
python quick_spoiler.py --test

# Interactive mode
python quick_spoiler.py

# Single example
python quick_spoiler.py \
    --post "You Won't Believe What This Celebrity Did Next" \
    --context "The celebrity announced retirement to pursue marine biology" \
    --strategy conservative
```

### 3. Batch Generation

```bash
# Generate spoilers cho toÃ n bá»™ validation set
python quick_spoiler.py \
    --batch data/validation.jsonl \
    --output validation_predictions.json \
    --strategy conservative
```

### 4. Evaluation (So sÃ¡nh vá»›i paper)

```bash
# ÄÃ¡nh giÃ¡ káº¿t quáº£ vá»›i metrics chuáº©n
python evaluate_spoilers.py \
    --predictions validation_predictions.json \
    --compare \
    --output detailed_evaluation.json
```

## ğŸ“ˆ Káº¿t quáº£ mong Ä‘á»£i

### Paper Results:
- **BLEU-4**: 0.58
- **ROUGE-L**: 0.68
- **BERTScore**: 0.46
- **METEOR**: 0.47

### Strategies Ä‘á»ƒ cáº£i thiá»‡n:

#### ğŸ¯ Conservative Strategy (Gáº§n paper nháº¥t)
```python
# Temperature = 0.0 (greedy)
# Beam size = 1 hoáº·c 4
# Max tokens = 30
```

#### ğŸ”„ Beam Search Strategy
```python
# Temperature = 0.0
# Beam size = 4
# No repeat ngram = 3
```

#### ğŸ¨ Creative Strategy (Äa dáº¡ng hÆ¡n)
```python
# Temperature = 0.7
# Top-p = 0.8
# Max tokens = 50
```

## ğŸ” PhÃ¢n tÃ­ch káº¿t quáº£

### Náº¿u BLEU/ROUGE tháº¥p nhÆ°ng BERTScore cao:
â¡ï¸ Model hiá»ƒu Ä‘Ãºng Ã½ nhÆ°ng diá»…n Ä‘áº¡t khÃ¡c â†’ Cáº§n generation conservative hÆ¡n

### Náº¿u káº¿t quáº£ quÃ¡ dÃ i:
â¡ï¸ Giáº£m `max_new_tokens`, tÄƒng `no_repeat_ngram_size`

### Náº¿u BLEU < 0.3:
â¡ï¸ Cáº§n training lÃ¢u hÆ¡n hoáº·c Ä‘iá»u chá»‰nh preprocessing

## ğŸ› ï¸ Debugging & Optimization

### 1. Kiá»ƒm tra data preprocessing
```bash
# Xem sample data
head -1 data/train.jsonl | python -m json.tool
```

### 2. Monitor training
```bash
# Tensorboard logs sáº½ Ä‘Æ°á»£c lÆ°u trong ./results/
tensorboard --logdir ./results
```

### 3. Test generation strategies
```bash
# So sÃ¡nh cÃ¡c strategies
python quick_spoiler.py --test
```

## ğŸ“ Examples

### Input:
```
Post: "Scientists Hate Him for This One Weird Discovery"
Context: "A researcher found that turmeric can reduce inflammation significantly."
```

### Expected Output:
```
Conservative: "turmeric reduces inflammation"
Beam: "turmeric can reduce inflammation"
Creative: "a common spice helps fight inflammation"
```

## âš¡ Quick Start

```bash
# 1. Clone vÃ  setup
cd clickbait-spoiling
pip install -r requirements.txt

# 2. Test ngay vá»›i pre-trained model
python quick_spoiler.py --test

# 3. Generate cho validation set
python quick_spoiler.py --batch data/validation.jsonl --output results.json

# 4. Evaluate
python evaluate_spoilers.py --predictions results.json --compare
```

## ğŸ¯ Tips Ä‘á»ƒ Ä‘áº¡t káº¿t quáº£ gáº§n paper

1. **Preprocessing**: Chá»‰ láº¥y paragraph ngáº¯n nháº¥t chá»©a answer
2. **Generation**: DÃ¹ng greedy hoáº·c beam nhá» (â‰¤4)
3. **Training**: 10 epochs vá»›i early stopping
4. **Evaluation**: Äáº£m báº£o lowercase + tokenization Ä‘Ãºng

## ğŸ“ Troubleshooting

### GPU Memory Issues:
```bash
# Giáº£m batch size
export CUDA_VISIBLE_DEVICES=0
# Hoáº·c sá»­ dá»¥ng gradient checkpointing
```

### Dependencies Issues:
```bash
# Reinstall transformers
pip install --upgrade transformers torch
```

### Low BLEU scores:
- Kiá»ƒm tra generation temperature (pháº£i = 0.0)
- Kiá»ƒm tra beam size (â‰¤4)
- Kiá»ƒm tra preprocessing input 